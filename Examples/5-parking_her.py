# -*- coding: utf-8 -*-
"""parking_her.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/eleurent/highway-env/blob/master/scripts/parking_her.ipynb

# Parking with Hindsight Experience Replay

##  Warming up
We start with a few useful installs and imports:
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

# Install environment and agent
# !pip install highway-env
# !pip install stable-baselines==2.10.0

# Environment
import gym
import highway_env

# Agent
from stable_baselines import HER, SAC

"""## Training"""

env = gym.make("parking-v0")
model = HER('MlpPolicy', env, SAC, n_sampled_goal=4,
            goal_selection_strategy='future',
            verbose=1, buffer_size=int(1e6),
            learning_rate=1e-3,
            gamma=0.9, batch_size=256,
            policy_kwargs=dict(layers=[256, 256, 256]))
model.learn(int(5e4))

"""## Visualize a few episodes

We first define a simple helper function for visualization of episodes:
"""

# !pip install gym pyvirtualdisplay
# !apt-get install -y xvfb python-opengl ffmpeg

from IPython import display as ipythondisplay
from pyvirtualdisplay import Display
from gym.wrappers import Monitor
from pathlib import Path
import base64
from tqdm.notebook import trange

# display = Display(visible=0, size=(1400, 900))
# display.start()

def show_video():
    html = []
    for mp4 in Path("video").glob("*.mp4"):
        video_b64 = base64.b64encode(mp4.read_bytes())
        html.append('''<video alt="{}" autoplay 
                      loop controls style="height: 400px;">
                      <source src="data:video/mp4;base64,{}" type="video/mp4" />
                 </video>'''.format(mp4, video_b64.decode('ascii')))
    ipythondisplay.display(ipythondisplay.HTML(data="<br>".join(html)))

"""
Test the policy"""

env = gym.make("parking-v0")
env = Monitor(env, './video', force=True, video_callable=lambda episode: True)
for episode in trange(3, desc="Test episodes"):
    obs, done = env.reset(), False
    env.unwrapped.automatic_rendering_callback = env.video_recorder.capture_frame
    while not done:
        action, _ = model.predict(obs)
        obs, reward, done, info = env.step(action)
env.close()
# show_video()